{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-o","title":"KIT-O","text":"<p>Lucas Levi Vaz</p> <p>Luiz Guilherme</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2 - Data 15/04/2025</li> <li> Roteiro 3 - Data 01/06/2025</li> <li> Roteiro 4 - Data 01/06/2025</li> <li> Projeto   - Data 01/06/2025</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"Projeto/main/","title":"Main","text":""},{"location":"Projeto/main/#objetivo","title":"Objetivo","text":"<p>O objetivo deste projeto \u00e9 criar uma API RESTful, dockeriz\u00e1-la e utiliz\u00e1-la via em nuvem via AWS Lightsail, incluindo a configura\u00e7\u00e3o de um banco de dados gerenciado e controle de custos operacionais, visando n\u00e3o estourar o custo mensal da infraestrutura de at\u00e9 US$:50,00</p>"},{"location":"Projeto/main/#api","title":"API","text":"<p>O primeiro passo foi desenvolver uma API RESTful com funcionalidades de cadastro, autentica\u00e7\u00e3o de usu\u00e1rios e consulta de dados protegidos por token JWT. Seu c\u00f3digo foi criado utilizando PostgreSQL e testado com FastAPI.</p> <p>Print do Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB:</p> <p></p> <p>FastApi</p> <p>Ap\u00f3s garantir o funcionamento da api, ela foi dockerizada e sua imagem publicada no Docker Hub.</p>"},{"location":"Projeto/main/#aws-lightsail","title":"AWS Lightsail","text":"<p>No AWS Lightsail, foi criado um novo servi\u00e7o de container para hospedar a aplica\u00e7\u00e3o. No painel da plataforma, foi informado o nome da imagem hospedada no Docker Hub e configurada a porta de entrada como 8080, que \u00e9 a utilizada pela API. Ap\u00f3s o deploy do container, o Lightsail forneceu um endpoint p\u00fablico, que foi copiado e adicionado \u00e0 documenta\u00e7\u00e3o como URL principal da API.</p> <p>Al\u00e9m disso, foi criado um banco de dados gerenciado do tipo PostgreSQL dentro do Lightsail. Durante a cria\u00e7\u00e3o, foram definidos nome da inst\u00e2ncia, nome do banco, usu\u00e1rio e senha. Com o banco dispon\u00edvel, o endpoint p\u00fablico fornecido foi copiado e utilizado na configura\u00e7\u00e3o das vari\u00e1veis de ambiente do container, possibilitando a conex\u00e3o entre a aplica\u00e7\u00e3o e o banco de dados em nuvem. Ap\u00f3s essas etapas, a API p\u00f4de ser acessada e testada com sucesso pela internet.</p> <p>Print do service no Lighsail:</p> <p></p> <p>Lighsail</p>"},{"location":"Projeto/main/#diagrama","title":"Diagrama","text":"<p>Diagrama do projeto:</p> <p></p> <p>Lighsail</p>"},{"location":"Projeto/main/#como-executar","title":"Como executar","text":"<p>Para executar basta copiar o endpoint p\u00fablico do container criado no Lighsail da seguinte maneira:</p> http://fastapi-service.tkg22yejn0r8w.us-east-2.cs.amazonlightsail.com/docs <p>Link do v\u00eddeo testando utilizando FastAPI:</p> <p>https://youtu.be/qDmwYlI38pM</p>"},{"location":"Projeto/main/#custos","title":"Custos","text":"<p>Print dos custos:</p> <p></p> <p>Custo</p>"},{"location":"Projeto/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O objetivo do projeto foi alcan\u00e7ado com \u00eaxito, uma vez que a API desenvolvida em FastAPI foi corretamente dockerizada e implantada no servi\u00e7o de containers do Amazon Lightsail, operando com sucesso em ambiente de nuvem. A aplica\u00e7\u00e3o demonstrou pleno funcionamento, permitindo cadastro, autentica\u00e7\u00e3o e consulta de dados via endpoints documentados no Swagger. Al\u00e9m disso, a infraestrutura foi mantida com um custo inferior a 50 d\u00f3lares, atendendo aos requisitos de efici\u00eancia e viabilidade econ\u00f4mica estabelecidos para o projeto.</p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>O objetivo desse projeto \u00e9 transformar servidores f\u00edsicos em recursos gerenci\u00e1veis online, de forma semelhante a uma infraestrutura de nuvem, tornando a administra\u00e7\u00e3o da rede de hardwares mais eficiente, pr\u00e1tica e escal\u00e1vel.</p>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro1/main/#infra","title":"Infra","text":""},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Instalando o MAAS:</p> sudo snap install maas --channel=3.5/Stable <p> </p> <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<p>Acessando a m\u00e1quina remotamente:</p> ssh cloud@172.16.0.3 <p>Configurando o MaaS:</p> sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:///sudo maas createadmin <p>Depois disso foi realizado um NAT para permitir o acesso externo \"Rede Wi-fi Insper\" do computador ao servidor MAIN.</p>"},{"location":"roteiro1/main/#app","title":"App","text":""},{"location":"roteiro1/main/#tarefa-1_1","title":"Tarefa 1","text":"<p>Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional:</p> systemctl status postgresql <p>Acessivel na pr\u00f3pria maquina na qual ele foi implantado:</p> telnet 127.0.0.1 5432 <p>Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN:</p> ping 172.16.8.31 <p>Em qual porta este servi\u00e7o est\u00e1 funcionando:</p> ss -tulnptelnet 127.0.0.1 5432 <p></p> <p>Comandos no terminal</p> <p>A imagem do terminal acima mostra a verifica\u00e7\u00e3o do PostgreSQL rodando no server1. O comando systemctl status postgresql confirma que o servi\u00e7o est\u00e1 ativo no sistema operacional. O teste de conex\u00e3o local (telnet 127.0.0.1 5432) indica que o servi\u00e7o est\u00e1 acess\u00edvel na pr\u00f3pria m\u00e1quina. A conectividade externa foi validada com ping 172.16.8.31, garantindo que o servidor pode se comunicar com a m\u00e1quina MAIN. Por fim, o comando ss -tulnp confirma que o PostgreSQL est\u00e1 rodando na porta 5432, tornando-o acess\u00edvel para conex\u00f5es.</p>"},{"location":"roteiro1/main/#tarefa-2_1","title":"Tarefa 2","text":"<p>Do Dashboard do MAAS com as m\u00e1quinas:</p> <p></p> <p>Dashboard do MAAS</p> <p>Da aba images, com as imagens sincronizadas:</p> <p> </p> <p>Comission server1</p> <p>As imagens a seguir mostram a Aba de cada maquina mostrando os testes de hardware e commissioning com Status \"OK\":</p> <p> </p> <p>Comission server1</p> <p> </p> <p>Comission server2</p> <p> </p> <p>Comission server3</p> <p> </p> <p>Comission server4</p> <p> </p> <p>Comission server5</p>"},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<p>Tela do Dashboard do MAAS com o server1 e server2 e seus respectivos IPs:</p> <p></p> <p>Dashboard do MAAS</p> <p>Aplica\u00e7\u00e3o Django, provando que est\u00e1 conectado ao server2 apartir do seu ip para realizar a ponte:</p> <p></p> <p>Django conectado ao server</p> <p>Para a implementacao manual da aplicacao Django e banco de dados foi necess\u00e1rio subir uma aplica\u00e7\u00e3o ORM Django pr\u00e9 produzida e realizado todos os comandos direto no cli do terminal:</p> maas cloud machines allocate name=server2 <p>Ap\u00f3s solicitar a reserva da m\u00e1quina e realizar deploy do server2 via cli, foi clonado o reposit\u00f3rio dentro do SSH do server2 e instalado sh e testado acessando o servi\u00e7o na porta 8080 no terminal do maas:</p> wget http://172.16.8.32:8080/admin/"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<p>Print da tela do Dashboard do MAAS com o server1, server2 e server3 e seus respectivos IPs:</p> <p></p> <p>Dashboard do MAAS</p> <p>Print da aplicacao Django, provando que est\u00e1 conectado ao server2 apartir do seu ip via ssh:</p> <p></p> <p>Django server 2</p> <p>Print da aplicacao Django, provando que est\u00e1 conectado ao server3 apartir do seu ip via ssh:</p> <p></p> <p>Django server 3</p> <p>Diferente do caso na tarefa 2, na qual o Django foi instalado manualmente, desta vez foi utilizado o Ansible para instalar o framework que \u00e9 um processo mais r\u00e1pido e pr\u00e1tico, o Ansible j\u00e1 instala o Django automaticamente e, al\u00e9m disso, \u00e9 escal\u00e1vel, ou seja, \u00e9 poss\u00edvel instalar em v\u00e1rios servidores de uma s\u00f3 vez, o que \u00e9 extremamente \u00fatil em grandes datacenters. Por outro lado, o processo manual \u00e9 mais detalhado e tem um controle maior sobre o que est\u00e1 sendo feito, j\u00e1 que \u00e9 realizado passo a passo no teminal.</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<p>Print da tela do Dashboard do MAAS com o server1, server2, server3 e server4 Maquinas e seus respectivos IPs:</p> <p></p> <p>Dashboard do MAAS</p> <p>Ap\u00f3s modificar os arquivos views.py para distinguir cada um dos servidores, o Nginx foi utilizado para aplicar o load balancing, distribuindo o tr\u00e1fico entre os servidores. O Ngninx recebe a requisi\u00e7\u00e3o e estabelece para qual servidor enviar.</p> <p>Prints das respostas de cada request, provando que est\u00e1 conectado ao server 4, batendo no server2 e server3 respectivamente:</p> <p></p> <p>Resposta Django 2</p> <p></p> <p>Resposta Django 3</p> <p>Note que, as pontes foram realizadas com portas diferentes das utilizadas na tarefa 4</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Uma dificuldade foi come\u00e7ar de fato, mas depois que foi entendido a maneira que as tarefas s\u00e3o cobradas e como \u00e9 preciso pesquisar e discutir em dupla, a atividade come\u00e7ou a fluir. Vale ressaltar que a fabrica\u00e7\u00e3o do cabo de rede acabou dando uma dor de cabe\u00e7a desnecess\u00e1ria, foi preciso ser refeito mais de uma vez e acabou atrasando o projeto no in\u00edcio. Uma parte f\u00e1cil foi trabalhar no dashboard do MaaS, \u00e9 uma plataforma intuitiva e pr\u00e1tica de mexer.</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O objetivo do projeto foi alcan\u00e7ado ao transformar servidores f\u00edsicos em recursos gerenci\u00e1veis online. A implementa\u00e7\u00e3o do MaaS permitiu gerenciar os servidores, enquanto o uso do NGINX como proxy reverso garantiu o balanceamento de carga adequado entre eles. A aplica\u00e7\u00e3o Django foi configurada e conectada aos servidores, utilizando tanto instala\u00e7\u00e3o manual quanto com Ansible, provando a escalabilidade do processo. </p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#objetivo","title":"Objetivo","text":"<p>O objetivo desse projeto \u00e9 implementar, na rede de servidores, a ferramenta de visualiza\u00e7\u00e3o e monitoramento de dados: Grafana, com o banco Prometheus.</p>"},{"location":"roteiro2/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro2/main/#infra","title":"Infra","text":"<p>Foi realizado o provisionamento de infraestrutura utilizando o Juju como orquestrador de deploy integrado ao MAAS, permitindo o gerenciamento din\u00e2mico de servidores bare-metal.</p>"},{"location":"roteiro2/main/#app","title":"App","text":""},{"location":"roteiro2/main/#tarefa-1","title":"Tarefa 1","text":"<p>Tela do Dashboard do MAAS com as m\u00e1quinas seus respectivos IPs:</p> <p> </p> <p>Dashboard do MAAS</p>"},{"location":"roteiro2/main/#tarefa-2","title":"Tarefa 2","text":"<p>Foi criado uma pasta \"charms\" para baixar o Grafana e o Prometheus, feito o deploy de ambos e sua integra\u00e7\u00e3o. A figura mostra a tela do terminal com o Grafana como active:</p> <p> </p> <p>Terminal Grafana-active</p>"},{"location":"roteiro2/main/#tarefa-3","title":"Tarefa 3","text":"<p>Foi criado um dashboard dentro do Grafana e adicionado o Prometheus como source:</p> <p> </p> <p>Grafana com Prometheus como Source</p>"},{"location":"roteiro2/main/#tarefa-4","title":"Tarefa 4","text":"<p>Para acessar o Dashboard do Grafana pela rede do Insper, foi necess\u00e1rio fazer um t\u00fanel:</p> <p> </p> <p>Tela Dashboard Grafana pela Rede do Insper</p>"},{"location":"roteiro2/main/#tarefa-5","title":"Tarefa 5","text":"<p>Por fim, foi utilizado t\u00fanel para ascessar o dashboard do Juju. A tela a seguir mostra aplica\u00e7\u00f5es sendo gerenciadas pelo Juju:</p> <p> </p> <p>Tela aplica\u00e7\u00f5es gerenciadas pelo Juju</p>"},{"location":"roteiro2/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Uma dificuldade foi compreender como funcionava o sistema de containers que foi utlizado para a instala\u00e7\u00e3o dos softwares. Essa mudan\u00e7a do controlador, utilizando o comando juju-switch, ficou um pouco confuso no come\u00e7o. O roteiro foi curto, n\u00e3o havia comandos de terminal muito complexos, oque facilitou sua realiza\u00e7\u00e3o.</p>"},{"location":"roteiro2/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O objetivo foi alcan\u00e7ado, o Grafana foi integrado ao Prometheus para monitoramento na rede de servidores. As ferramentas foram instaladas e configuradas com aux\u00edlio do Juju e MAAS, com acesso remoto garantido por meio de t\u00faneis de rede. </p>"},{"location":"roteiro3/main/","title":"Roteiro 3","text":""},{"location":"roteiro3/main/#objetivo","title":"Objetivo","text":"<p>O objetivo desse projeto \u00e9 configurar e utilizar uma nuvem privada OpenStack, explorando redes virtuais SDN e automa\u00e7\u00e3o com Juju e Terraform. Dividido em cria\u00e7\u00e3o, configura\u00e7\u00e3o e uso da infraestrutura, o roteiro foca em implantar aplica\u00e7\u00f5es em VMs e gerenciar recursos como c\u00f3digo. O objetivo \u00e9 compreender a arquitetura e opera\u00e7\u00e3o de nuvens privadas</p>"},{"location":"roteiro3/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro3/main/#infra","title":"Infra","text":"<p>Realizamos a instala\u00e7\u00e3o de todas as depend\u00eancias do Openstack, verificando a implanta\u00e7\u00e3o de cada uma delas atrav\u00e9s do comando juju status</p>"},{"location":"roteiro3/main/#setup","title":"Setup","text":""},{"location":"roteiro3/main/#tarefa-1","title":"Tarefa 1","text":"<p>Primeiro, ser\u00e3o mostradas cada aba do dashboard do OpenStack antes de realizar a configura\u00e7\u00e3o</p> <p>Terminal com todas as aplica\u00e7\u00f5es instaladas:</p> juju status <p></p> <p>Juju status</p> <p>Dashboard do MAAS com as m\u00e1quinas:</p> <p></p> <p>Tela do Dashboard do MAAS</p> <p>Aba compute overview no OpenStack Dashboard:</p> <p></p> <p>Compute overview</p> <p>Aba compute instances no OpenStack Dashboard:</p> <p></p> <p>Compute instances</p> <p>Aba network topology no OpenStack Dashboard:</p> <p></p> <p>Net topology</p> <p>Ap\u00f3s isso realizamos a configura\u00e7\u00e3o do OpenStack, definindo a rede interna, externa, conex\u00e3o e cria\u00e7\u00e3o das inst\u00e2mcias</p>"},{"location":"roteiro3/main/#tarefa-2","title":"Tarefa 2","text":"<p>Dashboard do MAAS com as m\u00e1quinas:</p> <p></p> <p>Tela do Dashboard do MAAS</p> <p>Aba compute overview no OpenStack Dashboard:</p> <p></p> <p>Compute overview</p> <p>Aba compute instances no OpenStack Dashboard:</p> <p></p> <p>Compute instances</p> <p>Aba network topology no OpenStack Dashboard:</p> <p></p> <p>Net topology</p> <p>Ap\u00f3s feita a configura\u00e7\u00e3o, \u00e9 poss\u00edvel notar a cria\u00e7\u00e3o de uma inst\u00e2ncia jammy-1, cada inst\u00e2ncia \u00e9 criada da seguinte maneira:</p> <p>Uma imagem de boot foi importada para o Glance para ser utilizada na cria\u00e7\u00e3o de inst\u00e2ncias de servidor.</p> mkdir ~/cloud-imageswget http://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img \\   -O ~/cloud-images/jammy-amd64.img <p>Importar a imagem para o Glance com o nome escolhido (jammy-amd64 no exemplo).</p> mkdir ~/cloud-imageswget http://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img \\   -O ~/cloud-images/jammy-amd64.img <p>Cria\u00e7\u00e3o de uma flavor para definir o perfil de hardware das novas inst\u00e2ncias. O flavor m1.small foi configurado com os seguintes recursos.</p> openstack flavor create --ram 2048 --disk 20 --ephemeral 20 m1.small <p>Al\u00e9m disso, configuramos a rede externa para permitir a conectividade, para isso:</p> <p>Foi criada a rede externa ext_net.</p> openstack network create --external --share \\   --provider-network-type flat --provider-physical-network physnet1 \\   ext_net <p>Uma sub-rede, chamada ext_subnet, foi criada para a rede ext_net, utilizando os valores baseados no ambiente local.</p> openstack subnet create --network ext_net --no-dhcp \\   --gateway 172.16.7.1 --subnet-range 172.16.7.0/23 \\   --allocation-pool start=172.16.7.0,end=172.16.8.255 \\   ext_subnet <p>Por fim configuramos a rede interna:</p> <p>Uma rede interna privada, chamada user1_net, foi criada para uso espec\u00edfico do projeto, junto com uma sub-rede chamada user1_subnet.</p> openstack network create --internal user1_netopenstack subnet create --network user1_net \\   --subnet-range 192.169.0.0/24 \\   --allocation-pool start=192.169.0.10,end=192.169.0.99 \\   user1_subnet <p>Um roteador chamado user1_router foi criado, associado \u00e0 sub-rede user1_subnet e configurado para usar a rede p\u00fablica externa ext_net como gateway.</p> openstack router create user1_routeropenstack router add subnet user1_router user1_subnetopenstack router set user1_router --external-gateway ext_net"},{"location":"roteiro3/main/#tarefa-3","title":"Tarefa 3","text":"<p>Representa\u00e7\u00e3o da arquitetura de rede:</p> <p> </p> <p>Diagrama Rede</p>"},{"location":"roteiro3/main/#app","title":"App","text":""},{"location":"roteiro3/main/#tarefa-4","title":"Tarefa 4","text":"<p>Foi realizada a configura\u00e7\u00e3o das aplica\u00e7\u00f5es em m\u00e1quinas virtuais (VMs) no OpenStack, seguindo a topologia especificada, com 2 inst\u00e2ncias da API, 1 inst\u00e2ncia de banco de dados e 1 inst\u00e2ncia de Load Balancer (Nginx). O foco foi otimizar os flavors para desempenho razo\u00e1vel e baixo custo</p> <p>Representa\u00e7\u00e3o da topologia</p> <p> </p> <p>Diagrama Topologia</p> <p>Lista de VMs utilizadas com nome e IPs alocados:</p> <p> </p> <p>Lista de VMs</p> <p>Print do Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB:</p> <p> </p> <p>Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB</p> <p>Os prints a seguir demonstram em qual server (m\u00e1quina fisica) cada inst\u00e2ncia foi alocado pelo OpenStack:</p> <p> </p> <p>API1</p> <p> </p> <p>API2</p> <p> </p> <p>Database</p> <p> </p> <p>Nginx</p>"},{"location":"roteiro3/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O objetivo do projeto foi alcan\u00e7ado ao criar a gest\u00e3o de uma nuvem privada OpenStack, desde a configura\u00e7\u00e3o inicial da infraestrutura com MAAS, Juju e Ceph, at\u00e9 a implanta\u00e7\u00e3o de aplica\u00e7\u00f5es e automa\u00e7\u00e3o com Terraform.Foram configuradas redes p\u00fablicas e internas, inst\u00e2ncias para Load Balancer, API e banco de dados. A infraestrutura atendeu aos requisitos de custo e desempenho, preparando-nos para gerenciar ambientes como esse.</p>"},{"location":"roteiro4/main/","title":"Roteiro 4","text":""},{"location":"roteiro4/main/#objetivo","title":"Objetivo","text":"<p>O objetivo desse projeto \u00e9 aplicar os princ\u00edpios de Infraestrutura como C\u00f3digo (IaC) com o Terraform, incluindo a defini\u00e7\u00e3o de recursos e o gerenciamento de ciclo de vida da infraestrutura de forma repet\u00edvel e idempotente. Al\u00e9m disso, introduzir os conceitos de SLA (acordos de n\u00edvel de servi\u00e7o) e DR (recupera\u00e7\u00e3o de desastres), destacando sua import\u00e2ncia para sistemas cr\u00edticos, e explorar a estrutura de gest\u00e3o de identidades e acesso no OpenStack (Keystone)</p>"},{"location":"roteiro4/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro4/main/#infra","title":"Infra","text":"<p>Na se\u00e7\u00e3o de cria\u00e7\u00e3o da hierarquia de projetos, foi estabelecida uma separa\u00e7\u00e3o l\u00f3gica de recursos no OpenStack via Horizon Dashboard para evitar conflitos entre usu\u00e1rios. Inicialmente, criou-se o dom\u00ednio AlunosDomain (Identity &gt; Domains &gt; Create Domain). Em seguida, foram criados dois projetos, KitOLuiz e KitOLevi, associados ao dom\u00ednio (Identity &gt; Projects &gt; Create Project). Dois usu\u00e1rios, Luiz e Levi, foram adicionados (Identity &gt; Users &gt; Create User), vinculados aos respectivos projetos com pap\u00e9is administrativos, garantindo uma gest\u00e3o isolada e eficiente.</p>"},{"location":"roteiro4/main/#app","title":"App","text":"<p>Para a cria\u00e7\u00e3o da infraestrutura com IaC, foi utilizada a ferramenta Terraform para definir e provisionar recursos no OpenStack de forma automatizada. Criou-se uma estrutura de pastas no servidor MAIN, com arquivos de configura\u00e7\u00e3o para o provedor OpenStack, duas inst\u00e2ncias (instance1 e instance2), uma rede interna (network_1) com sub-rede, e um roteador conectado \u00e0 rede externa. As credenciais do usu\u00e1rio foram configuradas via arquivo openrc, garantindo autentica\u00e7\u00e3o e consist\u00eancia na implanta\u00e7\u00e3o dos recursos.</p>"},{"location":"roteiro4/main/#tarefa-1","title":"Tarefa 1","text":"<p>Abas Identy projects no OpenStack:</p> <p></p> <p>Identity projects</p> <p>Aba Identy users no OpenStack:</p> <p></p> <p>Identity users</p> <p>Abas compute overview no OpenStack:</p> <p></p> <p>Identity users</p> <p></p> <p>Identity users</p> <p>Abas compute instances no OpenStack:</p> <p></p> <p>Compute instances</p> <p></p> <p>Compute instances</p> <p>Abas network topology no OpenStack:</p> <p></p> <p>Network topology</p> <p></p> <p>Network topology</p>"},{"location":"roteiro4/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O objetivo do projeto foi alcan\u00e7ado ao criar a gest\u00e3o de uma nuvem privada OpenStack, desde a configura\u00e7\u00e3o inicial da infraestrutura com MAAS, Juju e Ceph, at\u00e9 a implanta\u00e7\u00e3o de aplica\u00e7\u00f5es e automa\u00e7\u00e3o com Terraform. Foram configuradas redes p\u00fablicas e internas, inst\u00e2ncias para Load Balancer, API e banco de dados, al\u00e9m de uma hierarquia de projetos e usu\u00e1rios no OpenStack. A infraestrutura atendeu aos requisitos de custo e desempenho.</p>"},{"location":"roteiro4/main/#criando-um-plano-de-disaster-recovery-e-sla","title":"Criando um plano de Disaster Recovery e SLA","text":"<p>Com base o que foi conclu\u00eddo com esse projeto, os pontos a seguir foram pensados considerando o cargo de CTO (Chief Technology Officer) de uma grande empresa com sede em v\u00e1rias capitais no Brasil e precisa implantar um sistema cr\u00edtico, de baixo custo e com dados sigilosos para a \u00e1rea operacional.</p>"},{"location":"roteiro4/main/#public-ou-private","title":"Public ou Private?","text":"<p>Pelo o que foi aprendido nesse roteiro, seria mais sensato implementar um sistema de Private Cloud, principalmente porque a empresa estar\u00e1 trabalhando com dados sigilosos para a \u00e1rea operacional. Apesar de ter custos iniciais mais altos, Privates Cloud, como o OpenStack, elimina o risco de exposi\u00e7\u00e3o de dados sens\u00edveis em ambientes multitenant (como na nuvem p\u00fablica) e permite uma arquitetura sob medida para as necessidades operacionais da empresa.</p>"},{"location":"roteiro4/main/#time-de-devops","title":"Time de DevOps","text":"<p>Para garantir a estabilidade, escalabilidade e seguran\u00e7a do novo sistema cr\u00edtico da \u00e1rea operacional, seria essencial a forma\u00e7\u00e3o de um time de DevOps. A fun\u00e7\u00e3o desse time seria atuar com os desenvolvedores e com a equipe de infraestrutura, com o objetivo de automatizar os processos de desenvolvimento e manuten\u00e7\u00e3o, afim de diminuir erros por falha humana e sistematizar o esquema de modo que as tarefas sejam realizado sempre da mesma maneira.</p>"},{"location":"roteiro4/main/#plano-de-dr-disaster-recovery-e-ha-alta-disponibilidade","title":"Plano de DR (Disaster Recovery) e HA (Alta Disponibilidade)","text":"<p>Mapeamento das Principais Amea\u00e7as:</p> <p>Falha de Hardware: Quebra de servidores ou storage, causando indisponibilidade. Ataques Cibern\u00e9ticos: Ransomware ou DDoS, comprometendo dados sigilosos. Erros Humanos: Configura\u00e7\u00f5es incorretas ou exclus\u00f5es acidentais. Desastres Naturais: Inunda\u00e7\u00f5es ou apag\u00f5es em sedes. Falhas de Rede: Interrup\u00e7\u00f5es no tr\u00e1fego entre sedes.</p> <p>A\u00e7\u00f5es Priorizadas para Recupera\u00e7\u00e3o:</p> <p>Imediato (0-1 hora): Ativar alertas via monitoramento, isolar sistemas afetados e restaurar snapshots de backup. Curto Prazo (1-4 horas): Reconfigurar redes e recriar inst\u00e2ncias automaticamente. M\u00e9dio Prazo (4-24 horas): Restaurar bancos de dados e validar aplica\u00e7\u00f5es com testes.</p> <p>Pol\u00edtica de Backup:</p> <p>Frequ\u00eancia: Backups di\u00e1rios incrementais e semanais completos. Armazenamento: Replica\u00e7\u00e3o em storage separado e outra sede. Reten\u00e7\u00e3o: 7 dias di\u00e1rios e 4 semanas semanais. Testes: Restaura\u00e7\u00e3o semanal para garantir integridade.</p> <p>Implementa\u00e7\u00e3o de Alta Disponibilidade (HA):</p> <p>Redund\u00e2ncia: Manter m\u00faltiplos n\u00f3s de computa\u00e7\u00e3o e armazenamento. Balanceamento: Usar Load Balancers para distribuir requisi\u00e7\u00f5es. Replica\u00e7\u00e3o: Configurar replica\u00e7\u00e3o s\u00edncrona de dados em bancos e storage. Monitoramento: Implantar ferramentas como Prometheus para alertas em tempo real. Failover: Automatizar a troca para n\u00f3s reserva em caso de falha.</p> <p>Essa estrat\u00e9gia assegura um ambiente robusto, resiliente e preparado para mitigar interrup\u00e7\u00f5es, alinhado \u00e0s necessidades operacionais cr\u00edticas da empresa.</p>"}]}